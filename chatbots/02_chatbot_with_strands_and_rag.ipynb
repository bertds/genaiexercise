{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Contextual-aware Chatbot with Strands SDK and RAG\n",
    "\n",
    "In this notebook we will build a chatbot using Strands SDK that automatically handles conversation history and uses context from documents through RAG (Retrieval Augmented Generation). Unlike traditional approaches that require manual session management with a database, Strands provides built-in conversation tracking.\n",
    "\n",
    "## Key Advantages of Strands SDK:\n",
    "- **Built-in conversation management** - No need for a database setup\n",
    "- **Automatic context preservation** - Maintains conversation state across interactions\n",
    "- **Simplified agent creation** - Less boilerplate code\n",
    "- **Custom tool integration** - Easy to add RAG capabilities with custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pprint\n",
    "from typing import List\n",
    "from strands import tool, Agent\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.model_selector import create_text_model_selector, create_embedding_model_selector\n",
    "\n",
    "# Create interactive model selector\n",
    "model_selector = create_text_model_selector().display()\n",
    "bedrock_model = model_selector.get_model_id()\n",
    "print(\"\\nðŸŽ¯ Select your preferred model above and run the cells below to see it in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session = boto3.session.Session()\n",
    "region = boto3_session.region_name or \"us-east-1\"\n",
    "\n",
    "# Get the selected model\n",
    "selected_model = model_selector.get_model_id()\n",
    "model_info = model_selector.get_model_info()\n",
    "\n",
    "print(f\"Using model: {model_info['name']} ({selected_model})\")\n",
    "print(f\"Description: {model_info['description']}\")\n",
    "\n",
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple local vector store to demonstrate the RAG pattern with Strands SDK\n",
    "\n",
    "The RAG pattern enhances Q&A systems by retrieving relevant document chunks before generating responses. When a user asks a question, the system performs a similarity search against a vector database containing document embeddings, retrieves the most relevant chunks, and includes them as context in the LLM prompt to generate more accurate, grounded answers.\n",
    "\n",
    "With Strands SDK, we can create custom tools that handle document retrieval and integrate them seamlessly with the agent's conversation capabilities.\n",
    "\n",
    "First let's load the 2022 Shareholder letter from Andy Jassy (pdf) and split it into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processing (same as before, but we'll use it with Strands tools)\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "\n",
    "def load_and_split_pdf(file_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Load PDF and split into chunks\"\"\"\n",
    "    # Read PDF\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    \n",
    "    # Simple text splitting\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            # Try to break at a sentence or word boundary\n",
    "            while end > start and text[end] not in '.!?\\n ':\n",
    "                end -= 1\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = end - chunk_overlap if end < len(text) else end\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "file_name = \"./data/AMZN-2022-Shareholder-Letter.pdf\"\n",
    "document_chunks = load_and_split_pdf(file_name)\n",
    "\n",
    "print(f\"Loaded {len(document_chunks)} chunks from the PDF\")\n",
    "print(\"First chunk preview:\")\n",
    "pp.pprint(document_chunks[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create an in-memory FAISS db to hold the chunks from the previous step, which we will use to retrieve the relevant context (RAG pattern) based on the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive embedding model selector\n",
    "embedding_model_selector = create_embedding_model_selector().display()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Select your preferred embedding model above and run the cells below to see it in action!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Get the selected embedding model\n",
    "selected_embedding_model = embedding_model_selector.get_model_id()\n",
    "embedding_model_info = embedding_model_selector.get_model_info()\n",
    "\n",
    "print(f\"Using embedding model: {embedding_model_info['name']} ({selected_embedding_model})\")\n",
    "\n",
    "# Create embeddings for document chunks\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Get embedding for a text using Bedrock\"\"\"\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=selected_embedding_model,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return np.array(response_body['embedding'])\n",
    "\n",
    "# Create embeddings for all chunks\n",
    "print(\"Creating embeddings for document chunks...\")\n",
    "embeddings = []\n",
    "for i, chunk in enumerate(document_chunks):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing chunk {i}/{len(document_chunks)}\")\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Create FAISS index\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"Created FAISS index with {index.ntotal} vectors of dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a custom retrieval tool for Strands that will search the document chunks and provide context for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def document_retriever(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search through Amazon's 2022 Shareholder Letter to find relevant information.\n",
    "    This tool helps answer questions about Amazon's business, performance, and strategy.\n",
    "    \"\"\"\n",
    "    # Get embedding for the query\n",
    "    query_embedding = get_embedding(query).reshape(1, -1).astype('float32')\n",
    "    \n",
    "    # Search for similar chunks\n",
    "    k = 3  # Number of chunks to retrieve\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Get the relevant chunks\n",
    "    relevant_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(document_chunks):\n",
    "            relevant_chunks.append(f\"Context {i+1}: {document_chunks[idx]}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(relevant_chunks)\n",
    "\n",
    "\n",
    "# Test the retrieval tool\n",
    "test_query = \"What is Graviton?\"\n",
    "retrieved_context = document_retriever(test_query)\n",
    "print(\"Retrieved context for 'What is Graviton?':\")\n",
    "print(retrieved_context[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Strands Agent with RAG capabilities\n",
    "\n",
    "Now we'll create a conversational agent that can both maintain conversation history and retrieve relevant information from documents. This combines the best of both worlds - contextual conversation and knowledge retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversational RAG agent with Strands\n",
    "rag_agent = Agent(\n",
    "    system_prompt=\"\"\"You are an assistant for question-answering tasks about Amazon's business and strategy. \n",
    "    Use the document_retriever tool to search for relevant information from Amazon's 2022 Shareholder Letter when answering questions.\n",
    "    \n",
    "    When a user asks a question:\n",
    "    1. Use the document_retriever tool to find relevant context\n",
    "    2. Answer the question based on the retrieved information\n",
    "    3. If you don't know the answer even after searching, say that you don't know\n",
    "    4. Keep answers concise (3 sentences maximum) but informative\n",
    "    5. Maintain conversation context - if a follow-up question refers to previous topics, understand the connection\n",
    "    \n",
    "    You automatically maintain conversation history, so users can ask follow-up questions that reference previous topics.\"\"\",\n",
    "    tools=[document_retriever],\n",
    "    model=bedrock_model,\n",
    "    callback_handler=None,  # default is PrintingCallbackHandler\n",
    ")\n",
    "\n",
    "print(\"RAG Agent created with built-in conversation history and document retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the conversational RAG agent\n",
    "\n",
    "Now let's test our agent with questions that demonstrate both its retrieval capabilities and conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG agent with a question about Graviton\n",
    "question1 = \"What is Graviton?\"\n",
    "answer1 = rag_agent(question1)\n",
    "print(f\"Question: {question1}\")\n",
    "print(f\"Answer: {answer1}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation memory with a follow-up question\n",
    "# The agent should understand that \"they\" refers to Graviton processors from the previous question\n",
    "question2 = \"How much better price-performance do they deliver?\"\n",
    "answer2 = rag_agent(question2)\n",
    "print(f\"Question: {question2}\")\n",
    "print(f\"Answer: {answer2}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with another topic\n",
    "question3 = \"What are Amazon's key investments in AWS?\"\n",
    "answer3 = rag_agent(question3)\n",
    "print(f\"Question: {question3}\")\n",
    "print(f\"Answer: {answer3}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation context with a follow-up\n",
    "question4 = \"How do these investments compare to the previous year?\"\n",
    "answer4 = rag_agent(question4)\n",
    "print(f\"Question: {question4}\")\n",
    "print(f\"Answer: {answer4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
