{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Q&A application using Amazon Bedrock Knowledge Bases with Strands Agents\n",
    "\n",
    "### Context\n",
    "\n",
    "In this notebook, we will dive deep into building a Q&A application using Amazon Bedrock Knowledge Bases - Retrieve API. Here, we will query the Knowledge Base to get the desired number of document chunks based on similarity search. We will then augment the prompt with relevant documents. The prompt will be the input to Amazon Nova models for generating the response.\n",
    "\n",
    "With a Knowledge Base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company\n",
    "data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant,\n",
    "context-speciﬁc, and accurate responses without continuously retraining the FM. All information retrieved from\n",
    "Knowledge Bases comes with source attribution to improve transparency and minimize hallucinations. For more information on creating a Knowledge Base using console, please refer to this [post](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html).\n",
    "\n",
    "We will cover 2 parts in the notebook:\n",
    "\n",
    "- Part 1: We will use a Strands-based retrieve-first approach that maintains deterministic behavior.\n",
    "- Part 2: We will showcase the Strands SDK integration with custom tools.\n",
    "\n",
    "### Pattern\n",
    "\n",
    "We will implement the solution using Retreival Augmented Generation (RAG) pattern. RAG retrieves data from outside the language model and augments the prompts by adding the relevant retrieved data in context. Here, we are performing RAG effectively on the Knowledge Base created using console/sdk. \n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and ingested in vector database. Notebook [01_create_ingest_documents_test_kb.ipynb](./01_create_ingest_documents_test_kb.ipynb) takes care of it for you.\n",
    "\n",
    "1. Load the documents into the Knowledge Base by connecting your s3 bucket (data source). \n",
    "2. Ingestion - Knowledge Bases will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store.\n",
    "\n",
    "#### Use case\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on. This data is already ingested into the Amazon Bedrock Knowledge Bases. You will need the `Knowledge Base id` to run this example.\n",
    "In your specific use case, you can sync different files for different domain topics and query this notebook in the same manner to evaluate model responses using the retrieve API from Knowledge Bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "boto3_session = boto3.Session()\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\", config=bedrock_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2717f6883112442aa886f8efbdee122e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Model:', layout=Layout(width='600px'), options=(('Nova Lite - Fast, cost-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive model selector\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.model_selector import create_text_model_selector\n",
    "model_selector = create_text_model_selector().display()\n",
    "selected_model = model_selector.get_model_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Strands-Based Retrieve-First Implementation\n",
    "\n",
    "This section demonstrates a Strands-based retrieve-first function that maintains deterministic behavior while using Strands' cleaner API and better model management.\n",
    "\n",
    "In this approach we:\n",
    "\n",
    "1. **Always retrieve context first** - Deterministic retrieval behavior\n",
    "2. **Then prompt the LLM** - Generate response with retrieved context\n",
    "3. **Use explicit workflow control** - No agent decision-making about when to retrieve\n",
    "\n",
    "### Benefits of Retrieve-First Pattern:\n",
    "\n",
    "- **Deterministic Behavior** - Always retrieves relevant context before generating responses\n",
    "- **Explicit Control** - You control exactly when and how retrieval happens\n",
    "- **Consistent Context** - Every response is grounded in retrieved documents\n",
    "- **Reduced Hallucination** - Responses are always based on factual retrieved content\n",
    "- **Simplified Architecture** - Clear separation between retrieval and generation steps\n",
    "\n",
    "This pattern follows the approach described in the Strands Agents Knowledge Base example, where retrieval happens first, then the LLM generates a response based on the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the text chunks from the retrieveAPI response\n",
    "\n",
    "In the cell below, we will fetch the context from the retrieval results.\n",
    "\n",
    "You can view the associated `score` of each of the text chunk that was returned which depicts its correlation to the query in terms of how closely it matches it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch context from the response\n",
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        contexts.append(retrievedResult['content']['text'])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bedrock model\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=selected_model,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "def retrieve_context(query, num_results=4):\n",
    "    response = bedrock_agent_client.retrieve(\n",
    "        retrievalQuery={'text': query},\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': num_results,\n",
    "                'overrideSearchType': \"HYBRID\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    contexts = get_contexts(response['retrievalResults'])\n",
    "    return \"\\n\\n\".join(contexts)\n",
    "\n",
    "def answer_with_context(query):\n",
    "    context = retrieve_context(query)\n",
    "    # Simple agent call with context already included\n",
    "    agent = Agent(\n",
    "        system_prompt=\"You are a financial advisor AI system...\",\n",
    "        model=bedrock_model,\n",
    "        callback_handler=None,  # default is PrintingCallbackHandler\n",
    "    )\n",
    "    return agent(f\"Context: {context}\\n\\nQuestion: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Strands Retrieve-First Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Strands Retrieve-First Implementation ===\n",
      "Query: By what percentage did AWS revenue grow year-over-year in 2022?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer: AWS revenue grew 29% year-over-year in 2022 on a $62B revenue base.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with a single query\n",
    "test_query = \"By what percentage did AWS revenue grow year-over-year in 2022?\"\n",
    "\n",
    "print(\"=== Testing Strands Retrieve-First Implementation ===\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nProcessing...\")\n",
    "\n",
    "result = answer_with_context(test_query)\n",
    "print(f\"\\nAnswer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Strands SDK integration\n",
    "In this section, we will build a Q&A application using Retrieve API provided by Amazon Bedrock Knowledge Bases and Strands SDK. We will query the Knowledge Base to get the desired number of document chunks based on similarity search, create a custom retrieval tool with Strands Agent, and use Amazon Nova Lite model for answering questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom retrieval tool using Strands SDK that will call the `Retrieve API` provided by Amazon Bedrock Knowledge Bases. This tool converts user queries into embeddings, searches the Knowledge Base, and returns the relevant results, giving you more control to build custom workflows on top of the semantic search results. The output of the `Retrieve API` includes the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context:\n",
      "On the latter, we’re excited about seeing more next day and same-day deliveries, and we’re on track to have our fastest Prime delivery speeds ever in 2023. Overall, we remain confident about our plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.     AWS has an $85B annualized revenue run rate, is still early in its adoption curve, but at a juncture where it’s critical to stay focused on what matters most to customers over the long-haul. Despite growing 29% year-over- year (“YoY”) in 2022 on a $62B revenue base, AWS faces short-term headwinds right now as companies are being more cautious in spending given the challenging, current macroeconomic conditions. While some companies might obsess over how they could extract as much money from customers as possible in these tight times, it’s neither what customers want nor best for customers in the long term, so we’re taking a different tack. One of the many advantages of AWS and cloud computing is that when your business grows, you can seamlessly scale up; and conversely, if your business contracts, you can choose to give us back that capacity and cease paying for it. This elasticity is unique to the cloud, and doesn’t exist when you’ve already made expensive capital investments in your own on-premises datacenters, servers, and networking gear. In AWS, like all our businesses, we’re not trying to optimize for any one quarter or year. We’re trying to build customer relationships (and a business) that outlast all of us; and as a result, our AWS sales and support teams are spending much of their time helping customers optimize their AWS spend so they can better weather this uncertain economy. Many of these AWS customers tell us that they’re not cost-cutting as much as cost- optimizing so they can take their resources and apply them to emerging and inventive new customer experiences they’re planning. Customers have appreciated this customer-focused, long-term approach, and we think it’ll bode well for both customers and AWS.     While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing in AWS. Our new customer pipeline is robust, as are our active migrations.\n",
      "\n",
      "Many concluded that they didn’t want to continue managing their technology infrastructure themselves, and made the decision to accelerate their move to the cloud. This shift by so many companies (along with the economy recovering) helped re-accelerate AWS’s revenue growth to 37% YoY in 2021.     Conversely, our Consumer revenue grew dramatically in 2020. In 2020, Amazon’s North America and International Consumer revenue grew 39% YoY on the very large 2019 revenue base of $245 billion; and, this extraordinary growth extended into 2021 with revenue increasing 43% YoY in Q1 2021. These are astounding numbers. We realized the equivalent of three years’ forecasted growth in about 15 months.     As the world opened up again starting in late Q2 2021, and more people ventured out to eat, shop, and travel, consumer spending returned to being spread over many more entities. We weren’t sure what to expect in 2021, but the fact that we continued to grow at double digit rates (with a two-year Consumer compounded annual growth rate of 29%) was encouraging as customers appreciated the role Amazon played for them during the pandemic, and started using Amazon for a larger amount of their household purchases.     This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building a very large fulfillment network, and then had to double it in the last 24 months to meet customer demand. As we were bringing this new capacity online, the labor market tightened considerably, making it challenging both to receive all of the inventory our vendors and sellers wanted to send us and to place that inventory as close to customers as we typically do. Combined with ocean, air, and trucking capacity becoming scarcer and more expensive, this created extra transportation and productivity costs. Supply chains were disrupted in ways none of us had seen previously. We hoped that the major impact from COVID-19 would recede as 2021 drew to a close, but then omicron reared its head in December, which had worldwide ramifications, including impacting people’s ability to work. And then in late February, with Russia’s invasion of Ukraine, fuel costs and inflation became bigger issues with which to contend.     So, 2021 was a crazy and unpredictable year, continuing a trend from 2020. But, I’m proud of the incredible commitment and effort from our employees all over the world.\n",
      "\n",
      "But, there are two relatively simple statistics that underline our immense future opportunity. While we have a consumer business that’s $434B in 2022, the vast majority of total market segment share in global retail still resides in physical stores (roughly 80%). And, it’s a similar story for Global IT spending, where we have AWS revenue of $80B in 2022, with about 90% of Global IT spending still on-premises and yet to migrate to the cloud. As these equations steadily flip—as we’re already seeing happen—we believe our leading customer experiences, relentless invention, customer focus, and hard work will result in significant growth in the coming years. And, of course, this doesn’t include the other businesses and experiences we’re pursuing at Amazon, all of which are still in their early days.     I strongly believe that our best days are in front of us, and I look forward to working with my teammates at Amazon to make it so.     Sincerely,     Andy Jassy President and Chief Executive Officer Amazon.com, Inc.     P.S. As we have always done, our original 1997 Shareholder Letter follows. What’s written there is as true today as it was in 1997.1997 LETTER TO SHAREHOLDERS (Reprinted from the 1997 Annual Report)     To our shareholders:     Amazon.com passed many milestones in 1997: by year-end, we had served more than 1.5 million customers, yielding 838% revenue growth to $147.8 million, and extended our market leadership despite aggressive competitive entry.     But this is Day 1 for the Internet and, if we execute well, for Amazon.com. Today, online commerce saves customers money and precious time. Tomorrow, through personalization, online commerce will accelerate the very process of discovery. Amazon.com uses the Internet to create real value for its customers and, by doing so, hopes to create an enduring franchise, even in established and large markets.     We have a window of opportunity as larger players marshal the resources to pursue the online opportunity and as customers, new to purchasing online, are receptive to forming new relationships. The competitive landscape has continued to evolve at a fast pace. Many large players have moved online with credible offerings and have devoted substantial energy and resources to building awareness, traffic, and sales. Our goal is to move quickly to solidify and extend our current position while we begin to pursue the online commerce opportunities in other areas.\n",
      "\n",
      "The difficult part of this estimation exercise is that the direct cost reduction is the smallest portion of the customer benefit of moving to the cloud. The bigger benefit is the increased speed of software development – something that can significantly improve the customer’s competitiveness and top line. We have no reasonable way of estimating that portion of customer value except to say that it’s almost certainly larger than the direct cost savings. To be conservative here (and remembering we’re really only trying to get ballpark estimates), I’ll say it’s the same and call AWS customer value creation $38 billion in 2020.     Adding AWS and consumer together gives us total customer value creation in 2020 of $164 billion.Summarizing: Shareholders $21B Employees $91B 3P Sellers $25B Customers $164B Total $301B     If each group had an income statement representing their interactions with Amazon, the numbers above would be the “bottom lines” from those income statements. These numbers are part of the reason why people work for us, why sellers sell through us, and why customers buy from us. We create value for them. And this value creation is not a zero-sum game. It is not just moving money from one pocket to another. Draw the box big around all of society, and you’ll find that invention is the root of all real value creation. And value created is best thought of as a metric for innovation.     Of course, our relationship with these constituencies and the value we create isn’t exclusively dollars and cents. Money doesn’t tell the whole story. Our relationship with shareholders, for example, is relatively simple. They invest and hold shares for a duration of their choosing. We provide direction to shareowners infrequently on matters such as annual meetings and the right process to vote their shares. And even then they can ignore those directions and just skip voting.     Our relationship with employees is a very different example. We have processes they follow and standards they meet. We require training and various certifications. Employees have to show up at appointed times. Our interactions with employees are many, and they’re fine-grained. It’s not just about the pay and the benefits. It’s about all the other detailed aspects of the relationship too.     Does your Chair take comfort in the outcome of the recent union vote in Bessemer? No, he doesn’t. I think we need to do a better job for our employees.\n"
     ]
    }
   ],
   "source": [
    "# Create a custom retrieval tool for Strands Agent\n",
    "@tool\n",
    "def knowledge_base_retriever(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from Amazon Bedrock Knowledge Base.\n",
    "    This tool searches the Knowledge Base and returns relevant context for answering questions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_client.retrieve(\n",
    "            retrievalQuery={'text': query},\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': 4,\n",
    "                    'overrideSearchType': \"SEMANTIC\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract and format the retrieved contexts\n",
    "        contexts = []\n",
    "        for result in response['retrievalResults']:\n",
    "            contexts.append(result['content']['text'])\n",
    "        \n",
    "        return \"\\n\\n\".join(contexts)\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving from Knowledge Base: {str(e)}\"\n",
    "\n",
    "# Test the retrieval tool\n",
    "query = \"By what percentage did AWS revenue grow year-over-year in 2022?\"\n",
    "retrieved_context = knowledge_base_retriever(query)\n",
    "print(\"Retrieved context:\")\n",
    "print(retrieved_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt specific to the model to personalize responses\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. The prompt instructs the agent to use the `knowledge_base_retriever` tool that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a financial advisor AI system that provides answers to questions by using fact-based and statistical information when possible.\n",
    "Use the retriever tool to search for relevant information to answer user questions.\n",
    "\n",
    "Retrieval guidelines:\n",
    "- Only call the retriever tool ONCE per user question\n",
    "- If the first retrieval doesn't contain the exact answer, work with the information provided\n",
    "- Do NOT retry the same or similar queries - analyze what you received first\n",
    "- If the retrieved information is insufficient, clearly state what information is missing\n",
    "\n",
    "If the Knowledge Base does not contain the answer, and you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "The response should be specific and use statistics or numbers when possible.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Strands Agent with the retrieval tool\n",
    "financial_advisor_agent = Agent(\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[knowledge_base_retriever],\n",
    "    model=bedrock_model,\n",
    "    callback_handler=None,  # default is PrintingCallbackHandler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Strands Agent to answer questions with Knowledge Base retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: By what percentage did AWS revenue grow year-over-year in 2022?\n",
      "\n",
      "Answer:\n",
      "<thinking>The retrieved information contains the necessary data to answer the question. The document states that AWS grew 29% year-over-year in 2022 on a $62B revenue base.</thinking>\n",
      "\n",
      "The AWS revenue grew by 29% year-over-year in 2022.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the financial advisor agent with a question\n",
    "query = \"By what percentage did AWS revenue grow year-over-year in 2022?\"\n",
    "answer = financial_advisor_agent(query)\n",
    "print(\"Question:\", query)\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are Amazon's key investments in generative AI?\n",
      "\n",
      "Answer:\n",
      "<thinking>The retrieved information contains details about Amazon's investments in generative AI. It mentions that Amazon has been working on its own Large Language Models (LLMs) and believes that generative AI will transform and improve virtually every customer experience. Additionally, it states that AWS is democratizing this technology so companies of all sizes can leverage generative AI, offering price-performant machine learning chips and applications like AWS's CodeWhisperer.</thinking>\n",
      "\n",
      "Amazon's key investments in generative AI include:\n",
      "\n",
      "1. **Development of Large Language Models (LLMs):** Amazon has been working on its own LLMs, which are believed to transform and improve customer experiences.\n",
      "2. **Democratization of Generative AI:** AWS is making generative AI technology accessible to companies of all sizes by offering price-performant machine learning chips and applications.\n",
      "3. **Machine Learning Services:** AWS offers a broad range of machine learning services, including specialized chips for machine learning training and inference.\n",
      "4. **Applications:** AWS has developed applications like CodeWhisperer, which generates code suggestions in real time to revolutionize developer productivity.\n",
      "\n",
      "These investments reflect Amazon's commitment to advancing generative AI and making it accessible to a wide range of users and businesses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with another question to demonstrate the agent's capabilities\n",
    "query2 = \"What are Amazon's key investments in generative AI?\"\n",
    "answer2 = financial_advisor_agent(query2)\n",
    "print(\"Question:\", query2)\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We showed different retrieval techniques for customizing your RAG based application. Each approach offers distinct advantages:\n",
    "\n",
    "**Retrieve-First Pattern (Part 1):**\n",
    "\n",
    "- **Deterministic behavior** - Always retrieves context before generating responses\n",
    "- **Explicit workflow control** - You control exactly when retrieval happens\n",
    "- **Consistent grounding** - Every response is based on retrieved factual content\n",
    "- **Production reliability** - Predictable, controllable RAG workflows\n",
    "\n",
    "**Agent-Based Tools (Part 2):**\n",
    "\n",
    "- **Dynamic decision-making** - Agent decides when and how to use tools\n",
    "- **Simplified orchestration** - Strands SDK handles tool coordination automatically\n",
    "- **Conversational capabilities** - Natural interaction with built-in context management\n",
    "- **Extensible architecture** - Easy to add multiple tools and complex workflows\n",
    "\n",
    "### Recommendation:\n",
    "\n",
    "- **Use retrieve-first pattern** (Part 1) when you need deterministic, controllable RAG workflows with consistent context grounding\n",
    "- **Use agent-based tools** (Part 2) when you need dynamic decision-making, conversational flows, or complex tool orchestration"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
